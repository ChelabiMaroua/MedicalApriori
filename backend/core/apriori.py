from collections import defaultdict
from itertools import combinations
import numpy as np

class Apriori:
    def __init__(self, min_support=None, min_confidence=0.6):
        self.min_support = min_support
        self.min_confidence = min_confidence
        self.transactions = []
        self.frequent_itemsets = {}
        self.rules = []
        self.n_transactions = 0
        
    def fit(self, transactions):
        """Ex√©cuter l'algorithme Apriori"""
        self.transactions = transactions
        self.n_transactions = len(transactions)
        
        if self.n_transactions == 0:
            raise ValueError("‚ùå Aucune transaction fournie")
        
        # Calcul automatique du support si non d√©fini
        if self.min_support is None:
            self.calculate_adaptive_support()
        
        print(f"\nüîç D√©marrage Apriori")
        print("="*70)
        
        # √âtape 1 : itemsets de taille 1
        item_counts = defaultdict(int)
        for transaction in transactions:
            for item in transaction:
                item_counts[frozenset([item])] += 1
        
        frequent_1 = {
            itemset: count / self.n_transactions
            for itemset, count in item_counts.items()
            if (count / self.n_transactions) >= self.min_support
        }
        
        if not frequent_1:
            print("‚ö†Ô∏è  Aucun itemset fr√©quent trouv√©. Essayez de r√©duire min_support.")
            return self
        
        self.frequent_itemsets[1] = [
            {'itemset': list(itemset), 'support': sup}
            for itemset, sup in frequent_1.items()
        ]
        
        print(f"üìä It√©ration 1: {len(frequent_1)} itemsets fr√©quents trouv√©s")
        
        # √âtapes suivantes
        k = 2
        current_frequent = frequent_1
        while current_frequent:
            candidates = self._generate_candidates(current_frequent, k)
            if not candidates:
                break
            
            candidate_counts = defaultdict(int)
            for transaction in self.transactions:
                tset = set(transaction)
                for candidate in candidates:
                    if candidate.issubset(tset):
                        candidate_counts[candidate] += 1
            
            # üî• On calcule tous les supports bruts √† cette it√©ration
            all_supports = np.array([count / self.n_transactions for count in candidate_counts.values()])
            
            # üî• Recalcul du min_support dynamique selon la distribution actuelle
            if len(all_supports) > 0:
                mean_support = np.mean(all_supports)
                std_support = np.std(all_supports)
                
                # On √©carte les valeurs extr√™mes (au-del√† de 2 √©carts-types)
                filtered_supports = [s for s in all_supports if abs(s - mean_support) <= 2 * std_support]
                
                if len(filtered_supports) > 0:
                    adjusted_mean = np.mean(filtered_supports)
                else:
                    adjusted_mean = mean_support
                
                # On ajuste le min_support avec un facteur de stabilit√©
                prev_support = self.min_support
                self.min_support = max(0.01, min(0.5, 0.5 * prev_support + 0.5 * adjusted_mean))
                
                print(f"‚öôÔ∏è  Support recalcul√© √† l‚Äôit√©ration {k}: moyenne={adjusted_mean:.4f}, œÉ={std_support:.4f} ‚Üí min_support={self.min_support:.4f}")
            
            new_frequent = {
                itemset: count / self.n_transactions
                for itemset, count in candidate_counts.items()
                if (count / self.n_transactions) >= self.min_support
            }
            
            if not new_frequent:
                break
            
            self.frequent_itemsets[k] = [
                {'itemset': list(itemset), 'support': sup}
                for itemset, sup in new_frequent.items()
            ]
            
            print(f"üìä It√©ration {k}: {len(new_frequent)} itemsets fr√©quents trouv√©s")
            current_frequent = new_frequent
            k += 1
        
        print(f"\n‚úÖ Apriori termin√© ({k-1} it√©rations)")
        print(f"üì¶ Total itemsets fr√©quents: {sum(len(v) for v in self.frequent_itemsets.values())}")
        return self

    def calculate_adaptive_support(self):
        """Calcul initial adaptatif du support bas√© sur les caract√©ristiques du dataset"""
        print("\n‚öôÔ∏è Calcul du support minimal adaptatif initial")
        print("="*70)
        
        avg_length = sum(len(t) for t in self.transactions) / len(self.transactions)
        unique_items = len(set(item for t in self.transactions for item in t))
        density = avg_length / unique_items if unique_items > 0 else 0
        
        # Heuristique initiale
        base_support = 0.02
        if self.n_transactions < 100:
            size_factor = 0.15
        elif self.n_transactions < 500:
            size_factor = 0.08
        else:
            size_factor = 0.03
        density_factor = max(0.01, min(0.1, density * 0.5))
        support = max(base_support, min(0.3, size_factor + density_factor))
        
        self.min_support = support
        print(f"‚úÖ Support initial calcul√©: {support:.4f}")
        return support

    # Autres m√©thodes (_generate_candidates, _has_frequent_subsets, _get_support, generate_rules, analyze_results, etc.)
    # inchang√©es
    
    def _generate_candidates(self, frequent_itemsets, k):
        """G√©n√®re les candidats de taille k selon Apriori"""
        candidates = set()
        itemsets_list = sorted([tuple(sorted(itemset)) for itemset in frequent_itemsets.keys()])
        
        for i in range(len(itemsets_list)):
            for j in range(i+1, len(itemsets_list)):
                itemset1 = itemsets_list[i]
                itemset2 = itemsets_list[j]
                # V√©rification du pr√©fixe commun
                if itemset1[:-1] == itemset2[:-1]:
                    new_candidate = frozenset(itemset1 + (itemset2[-1],))
                    # √âlagage bas√© sur la propri√©t√© antimonotone
                    if self._has_frequent_subsets(new_candidate, frequent_itemsets, k):
                        candidates.add(new_candidate)
        return candidates
    
    def _has_frequent_subsets(self, candidate, frequent_itemsets, k):
        """V√©rifie que tous les sous-ensembles de taille k-1 sont fr√©quents"""
        for item in candidate:
            subset = candidate - frozenset([item])
            if subset not in frequent_itemsets:
                return False
        return True
    
    def _get_support(self, itemset):
        """Retourne le support d'un itemset"""
        k = len(itemset)
        if k in self.frequent_itemsets:
            for data in self.frequent_itemsets[k]:
                if frozenset(data['itemset']) == itemset:
                    return data['support']
        return 0
    
    def generate_rules(self):
        """G√©n√®re les r√®gles d'association avec support et confiance"""
        print("\nüéØ G√©n√©ration des r√®gles d'association")
        print("="*70)
        self.rules = []
        
        for k, itemsets in self.frequent_itemsets.items():
            if k < 2:
                continue
            for data in itemsets:
                itemset = frozenset(data['itemset'])
                itemset_support = data['support']
                
                # G√©n√©ration de toutes les r√®gles possibles
                for i in range(1, len(itemset)):
                    for antecedent in combinations(itemset, i):
                        antecedent = frozenset(antecedent)
                        consequent = itemset - antecedent
                        antecedent_support = self._get_support(antecedent)
                        
                        if antecedent_support == 0:
                            continue
                        
                        confidence = itemset_support / antecedent_support
                        
                        if confidence >= self.min_confidence:
                            self.rules.append({
                                'antecedent': list(antecedent),
                                'consequent': list(consequent),
                                'support': itemset_support,
                                'confidence': confidence
                            })
        
        # Tri par confiance puis support
        self.rules.sort(key=lambda x: (-x['confidence'], -x['support']))
        print(f"‚úÖ {len(self.rules)} r√®gles g√©n√©r√©es")
        return self.rules
    
    def analyze_results(self, top_n=5):
        """Analyse d√©taill√©e des r√©sultats avec support et confiance"""
        print("\n" + "="*70)
        print(f"üìã ANALYSE DES R√âSULTATS (TOP {top_n})")
        print("="*70)
        
        if not self.rules:
            print("‚ùå Aucune r√®gle trouv√©e.")
            return
        
        # TOP N des r√®gles par confiance
        print(f"\nü•á TOP {top_n} R√àGLES PAR CONFIANCE :")
        print("-"*70)
        for i, rule in enumerate(self.rules[:top_n], 1):
            self._print_rule(rule, i)
        
        # TOP N des r√®gles par support
        print(f"\nüìä TOP {top_n} R√àGLES PAR SUPPORT :")
        print("-"*70)
        sorted_by_support = sorted(self.rules, key=lambda r: -r['support'])
        for i, rule in enumerate(sorted_by_support[:top_n], 1):
            self._print_rule(rule, i)
        
        # Statistiques globales
        print("\nüìä STATISTIQUES GLOBALES :")
        print("-"*70)
        confidences = [r['confidence'] for r in self.rules]
        supports = [r['support'] for r in self.rules]
        
        print(f"Nombre total de r√®gles: {len(self.rules)}")
        print(f"Confiance moyenne: {np.mean(confidences):.3f}")
        print(f"Confiance m√©diane: {np.median(confidences):.3f}")
        print(f"Confiance min/max: {min(confidences):.3f}/{max(confidences):.3f}")
        print(f"Support moyen: {np.mean(supports):.3f}")
        print(f"Support m√©dian: {np.median(supports):.3f}")
        print(f"Support min/max: {min(supports):.3f}/{max(supports):.3f}")
    
    def _print_rule(self, rule, index):
        """Affichage format√© d'une r√®gle"""
        antecedent = ' ET '.join(sorted(rule['antecedent']))
        consequent = ' ET '.join(sorted(rule['consequent']))
        
        print(f"\nüìå R√àGLE #{index}")
        print(f"   SI [{antecedent}]")
        print(f"   ‚Üí ALORS [{consequent}]")
        print(f"   üìä Support={rule['support']:.3f} | Confiance={rule['confidence']:.3f}")
    
    def export_results(self, filepath='apriori_results.csv'):
        """Exporte les r√©sultats vers un fichier CSV"""
        import pandas as pd
        
        if not self.rules:
            print("‚ö†Ô∏è  Aucune r√®gle √† exporter")
            return
        
        export_data = []
        for rule in self.rules:
            export_data.append({
                'Antecedent': ' & '.join(sorted(rule['antecedent'])),
                'Consequent': ' & '.join(sorted(rule['consequent'])),
                'Support': rule['support'],
                'Confidence': rule['confidence']
            })
        
        df = pd.DataFrame(export_data)
        df.to_csv(filepath, index=False, encoding='utf-8')
        print(f"‚úÖ R√©sultats export√©s vers: {filepath}")